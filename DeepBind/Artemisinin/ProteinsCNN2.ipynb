{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import Request, urlopen\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import glob\n",
    "from tensorboardX import SummaryWriter\n",
    "import pickle\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epochs = 1\n",
    "uniprot_xs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('tensorboard --logdir runs &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "#a = torch.cuda.FloatTensor(4, 4, 1) # create tensor on gpu\n",
    "#b = a.cpu() --- transfer a to cpu\n",
    "#c = b.numpy() --- turn into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['num_layers'] = []\n",
    "params['filter_sz'] = []\n",
    "params['num_filters'] = []\n",
    "params['learning_rate'] = []\n",
    "params['batch_sz'] = []\n",
    "params['epochs'] = []\n",
    "params['loss_last'] = []\n",
    "params['acc_last']= []\n",
    "params['lr_step'] = []\n",
    "params['lr_decay'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_acc(pred, label):\n",
    "    l = loss_func(pred, label).cuda()\n",
    "    pr = pred.data.cpu().numpy()\n",
    "    la = label.data.cpu().numpy()\n",
    "    a = np.mean(np.int32(np.equal(np.argmax(pr, 1), la)))\n",
    "    return l, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss, testloss, acc, testacc, num):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    a = fig.add_subplot(121)\n",
    "    a.plot(loss)\n",
    "    a.plot(np.arange(len(testloss))*num, testloss)\n",
    "    a.set_xlabel('Training Iteration')\n",
    "    a.set_ylabel('Loss')\n",
    "    b = fig.add_subplot(122)\n",
    "    b.plot(acc)\n",
    "    b.plot(np.arange(len(testacc))*num, testacc)\n",
    "    b.set_xlabel('Training Iteration')\n",
    "    b.set_ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param_dict(nl, fs, nf, lr, bs, ep, lrs, lrd):\n",
    "    params['num_layers'].append(nl)\n",
    "    params['filter_sz'].append(fs)\n",
    "    params['num_filters'].append(nf)\n",
    "    params['learning_rate'].append(lr)\n",
    "    params['batch_sz'].append(bs)\n",
    "    params['epochs'].append(ep)\n",
    "    params['lr_step'].append(lrs)\n",
    "    params['lr_decay'].append(lrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(x, y):\n",
    "    x = torch.from_numpy(x).cuda().float()\n",
    "    y = torch.from_numpy(y).cuda().long()\n",
    "    return Variable(x), Variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniprot_data(kw, numxs):\n",
    "    '''Goes to the uniprot website and searches for \n",
    "       data with the keyword given. Returns the data \n",
    "       found up to limit elements.'''\n",
    "\n",
    "    kws = [kw, 'NOT+' + kw]\n",
    "    Protein_data = {}\n",
    "            \n",
    "    for i in range(2):\n",
    "        kw = kws[i]\n",
    "        url1 = 'http://www.uniprot.org/uniprot/?query='\n",
    "        url2 = '&columns=sequence&format=tab&limit='+str(numxs)\n",
    "        query_complete = url1 + kw + url2\n",
    "        request = Request(query_complete)\n",
    "        response = urlopen(request)\n",
    "        data = response.read()\n",
    "        data = str(data, 'utf-8')\n",
    "        data = data.split('\\n')\n",
    "        data = data[1:-1]\n",
    "        Protein_data[str(i)] = list(map(lambda x:x.lower(),data))\n",
    "\n",
    "    x = Protein_data['0'] + Protein_data['1']\n",
    "    y = np.zeros([numxs*2, ])\n",
    "    y[:numxs] = 1.\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_strings(c):\n",
    "    longest = len(max(c, key=len))\n",
    "    digits = len(str(longest))\n",
    "    pad_num = np.ceil(longest*10**(-(digits-2)))\n",
    "    pad_num = int(pad_num * 10**(digits-2))\n",
    "    N = len(c)\n",
    "    X = np.zeros([N, 1, pad_num, 1])\n",
    "    m = 0\n",
    "            \n",
    "    for seq in c:\n",
    "        x = [] \n",
    "        for letter in seq:\n",
    "            x.append(max(ord(letter)-97, 0))\n",
    "                    \n",
    "        x = np.asarray(x)\n",
    "        diff = pad_num - x.size\n",
    "\n",
    "        if diff % 2 == 0:\n",
    "            x = np.pad(x, diff//2, \n",
    "                        'constant', constant_values=22.)\n",
    "        else:\n",
    "            x = np.pad(x, (int(np.floor(diff/2)), \n",
    "                        int(np.ceil(diff/2))), \n",
    "                        'constant', constant_values=22.)\n",
    "        \n",
    "        X[m, ...] = x[None, :, None]\n",
    "        m += 1\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_layers, fsize, num_filters, str_shp):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model_ops = {}\n",
    "        self.n_layers = n_layers \n",
    "        ch = 1\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            if i % 2 == 0 and i > 0:\n",
    "                num_filters *= 2\n",
    "            if i == 0: strides=2\n",
    "            else: strides=1\n",
    "                \n",
    "            conv = nn.Conv2d(ch,\n",
    "                             num_filters,\n",
    "                             (1, fsize),\n",
    "                             padding=(0, int(np.floor(fsize/2))),\n",
    "                             stride=(1, strides)).cuda()\n",
    "            self.model_ops['conv'+str(i)] = nn.DataParallel(conv)\n",
    "            ch = num_filters\n",
    "        #self.fc1 = nn.DataParallel(nn.Linear(ch, 2).cuda())\n",
    "        self.fc1 = nn.DataParallel(nn.Linear(ch*str_shp//2, 500).cuda())\n",
    "        self.fc2 = nn.DataParallel(nn.Linear(500, 2).cuda())\n",
    "        self.sm = nn.DataParallel(nn.Softmax(dim=1).cuda())\n",
    "        #self.fc2 = nn.DataParallel(nn.Linear(500, 2).cuda())\n",
    "        \n",
    "        \n",
    "    def forward(self, net):\n",
    "        for j in range(self.n_layers):\n",
    "            net = self.model_ops['conv'+str(j)](net)\n",
    "        #net = F.max_pool2d(net, kernel_size=net.size()[2:])\n",
    "        net = net.view(net.shape[0], -1).cuda(1)\n",
    "        net = self.fc1(net)\n",
    "        net = self.fc2(net)\n",
    "        return self.sm(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from uniprot and make labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_uniprot_data('homeobox', uniprot_xs)\n",
    "X = process_strings(X)\n",
    "X = np.transpose(X, (0, 3, 1, 2))\n",
    "assert(X.shape[0]==Y.shape[0]),'diff. nums of data & labels'\n",
    "\n",
    "# shuffle data\n",
    "randtrain = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "X = X[randtrain, ...]\n",
    "Y = Y[randtrain]\n",
    "\n",
    "# split data into training and testing\n",
    "train_num = int(0.8 * X.shape[0])\n",
    "testX = X[train_num:, ...]\n",
    "testY = Y[train_num:]\n",
    "X = X[:train_num, ...]\n",
    "Y = Y[:train_num]\n",
    "\n",
    "print('Train num: %d; Test num: %d'%(X.shape[0], testX.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(41):\n",
    "    Bar = ProgressBar() # initialize progressbar \n",
    "    writer = SummaryWriter('runs/train' + str(i))\n",
    "    writer2 = SummaryWriter('runs/test' + str(i))\n",
    "    \n",
    "    \n",
    "    # each iter use random point in parameter space\n",
    "    layers = int(np.random.randint(1, 8, 1)[0])\n",
    "    scale = np.random.uniform(.1, 10.) / np.random.randint(1, 1000, 1)[0]\n",
    "    learning_rate = np.absolute(np.random.randn(1)[0]) * scale\n",
    "    batch_sz = np.around(np.random.randint(10, 151, 1)[0])\n",
    "    fsizes = int(np.random.randint(2, 101, 1)[0])\n",
    "    if fsizes % 2 == 0: fsizes += 1\n",
    "    nf = int(np.random.randint(50, 351, 1)[0]) // (layers + 1)\n",
    "    epochs = int(np.random.randint(8, 61, 1)[0])\n",
    "    lr_step = np.random.randint(0, epochs, 1)[0]\n",
    "    lr_d  = np.random.uniform(1e-4, 1.)\n",
    "    update_param_dict(layers, fsizes, nf, learning_rate,\n",
    "                      batch_sz, epochs, lr_step, lr_d)\n",
    "    print(params)\n",
    "    \n",
    "    \n",
    "    # instantiate the network\n",
    "    Network = CNN(layers, fsizes, nf, X.shape[-1])\n",
    "    \n",
    "    # define loss function\n",
    "    loss_func = nn.CrossEntropyLoss().cuda(1)\n",
    "    \n",
    "    # define autodiff optimizer and initialize loss/acc lists\n",
    "    opt = optim.Adam(Network.parameters(), lr=learning_rate)\n",
    "    lr_decay = optim.lr_scheduler.StepLR(opt, step_size=lr_step, gamma=lr_d)\n",
    "    loss_ = []\n",
    "    acc_ = []\n",
    "    tloss_ = []\n",
    "    tacc_ = []\n",
    "    n_iters = int(np.ceil(X.shape[0] / batch_sz))\n",
    "    \n",
    "    for epoch in Bar(range(epochs)):\n",
    "        # shuffle data and labels\n",
    "        rand = np.random.permutation(X.shape[0])\n",
    "        X = X[rand, ...]\n",
    "        Y = Y[rand]\n",
    "        \n",
    "        for iters in range(X.shape[0]//batch_sz):\n",
    "            if batch_sz*(iters+1) < X.shape[0]:\n",
    "                x = X[iters*batch_sz:batch_sz*(iters+1), ...]\n",
    "                y = Y[iters*batch_sz:batch_sz*(iters+1)]\n",
    "            else:\n",
    "                x = X[iters*batch_sz:, ...]\n",
    "                y = Y[iters*batch_sz:]\n",
    "                \n",
    "            x, y = prepare_batch(x, y)\n",
    "        \n",
    "            # get the output of the network and compute loss/acc.\n",
    "            loss, acc = loss_acc(Network(x), y)\n",
    "            acc_.append(acc)\n",
    "            loss_.append(loss.data[0])\n",
    "            \n",
    "            # perform a step down the gradient\n",
    "            opt.zero_grad() # zero gradient\n",
    "            loss.backward() # accumulate gradient \n",
    "            lr_decay.step() # for lr decay #opt.step()\n",
    "            \n",
    "            writer.add_scalar('Train Acc.', acc, epoch*n_iters+iters)\n",
    "            writer2.add_scalar('Train Loss', loss.data[0], epoch*n_iters+iters)\n",
    "\n",
    "            \n",
    "        # test on validation set and plot loss/acc\n",
    "        if epoch % test_epochs == 0:\n",
    "            randtest = np.random.randint(0, testX.shape[0], 500)\n",
    "            tx, ty = prepare_batch(testX[randtest, ...], testY[randtest])\n",
    "            test_loss, test_acc = loss_acc(Network(tx), ty)\n",
    "            tloss_.append(test_loss.data[0])\n",
    "            tacc_.append(test_acc)\n",
    "            writer.add_scalar('Val. Acc.', test_acc, n_iters*epoch)\n",
    "            writer2.add_scalar('Val. Loss', test_loss.data[0], n_iters*epoch)\n",
    "            \n",
    "    tloss_ = np.mean(np.asarray(tloss_)[-6:])\n",
    "    tacc_ = np.mean(np.asarray(tacc_)[-6:])\n",
    "    params['loss_last'].append(tloss_)\n",
    "    params['acc_last'].append(tacc_)\n",
    "    #writer.close()\n",
    "    #writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('protein_hparams.pickle', 'wb') as handle:\n",
    "    pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
